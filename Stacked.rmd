---
title: "Stacked"
author: "Mathews, Chris"
date: '2022-06-24'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(tidymodels)
library(caret)
library(mice)
library(VIM)
library(ranger)
library(RColorBrewer)
library(rpart)
library(rattle)
library(e1071)
library(xgboost)
library(usemodels)
library(nnet) #our neural network package
library(stacks)
library(vip)
```


```{r}
ames <- readRDS("AMESFINAL.RDS")
#ames <- ames %>% filter(Kitchen_Qual != "Other") %>% filter(Bsmt_Qual != "Other")
```

Now we'll split the data.  
```{r}
set.seed(123) 
ames_split = initial_split(ames, prop = 0.7, strata = Above_Median) #70% in training
train = training(ames_split)
test = testing(ames_split)
```

Set-up our folds
```{r}
set.seed(123)
folds = vfold_cv(train, v = 5)
```

Let's build three models: A classification tree, a random forest, and an XGB model. First, some preliminaries.    
```{r}
ames_recipe = recipe(Above_Median ~., train) #set-up a basic recipe, we can add to it as needed

ctrl_grid = control_stack_grid() #necessary for working with the stacks package
ctrl_res = control_stack_resamples() #necessary for working with the stacks package
```

### Tree Model
Set-up the classification tree  
```{r}
tree_model = decision_tree(cost_complexity = tune()) %>% 
  set_engine("rpart", model = TRUE) %>% 
  set_mode("classification")

tree_recipe = ames_recipe %>%
  step_dummy(all_nominal(),-all_outcomes())

tree_workflow = workflow() %>%
  add_model(tree_model) %>%
  add_recipe(tree_recipe)

set.seed(1234)
tree_res = 
  tree_workflow %>% 
  tune_grid(
    resamples = folds,
    grid = 25, #try 25 reasonable values for cp
    control = ctrl_grid #needed for stacking
    )
```


Parameter tuning (iterative tuning)
```{r}
tree_res %>%
  collect_metrics() %>%
  ggplot(aes(cost_complexity, mean)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) 
```

### Random Forest Model  
This model takes awhile, so I've commented it out and saved the resamples to an RDS.  
```{r}
rf_recipe = tree_recipe %>%
   step_dummy(all_nominal(), -all_outcomes())
 
rf_model = rand_forest(mtry = tune(), min_n = tune(), trees = 200) %>% #add tuning of mtry and min_n parameters
   set_engine("ranger", importance = "permutation") %>% #added importance metric
   set_mode("classification")
 
rf_wflow = 
   workflow() %>% 
   add_model(rf_model) %>% 
   add_recipe(rf_recipe)
 
 set.seed(1234)
rf_res = tune_grid(
   rf_wflow,
   resamples = folds,
   grid = 200, 
   control = ctrl_grid
)
```

```{r}
rf_res %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  mutate(min_n = factor(min_n)) %>%
  ggplot(aes(mtry, mean, color = min_n)) +
  geom_line(alpha = 0.5, size = 1.5) +
  geom_point() +
  labs(y = "Accuracy")
```


```{r}
best_rf = select_best(rf_res, "accuracy")

final_rf = finalize_workflow(
  ames_wflow,
  best_rf
)

final_rf
```


### Neural Network Model
This model also takes awhile to run. Saved to RDS.  
```{r}
nn_recipe = ames_recipe %>%
  step_normalize(all_predictors(), -all_nominal()) #normalize the numeric predictors, not needed for categorical
 
nn_model =
   mlp(hidden_units = tune(), penalty = tune(),
       epochs = tune()) %>%
   set_mode("classification") %>%
   set_engine("nnet", verbose = 0) #verbose = 0 reduces output from the model
 
nn_workflow <-
   workflow() %>%
   add_recipe(nn_recipe) %>%
   add_model(nn_model)
 
set.seed(1234)
neural_res <-
 tune_grid(nn_workflow,
             resamples = folds,
             grid = 200,
             control = ctrl_grid)
```

```{r}
neural_res %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  select(mean, hidden_units, penalty, epochs) %>%
  pivot_longer(hidden_units:epochs,
    values_to = "value",
    names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "Accuracy")
```

```{r}
neural_res %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  mutate(hidden_units = factor(hidden_units)) %>%
  ggplot(aes(penalty, mean, color = epochs)) +
  geom_line(alpha = 0.5, size = 1.5) +
  geom_point() +
  facet_wrap(~hidden_units, ncol =2 ) + 
  labs(y = "Accuracy")
```
### XGB
```{r}
start_time = Sys.time() #for timing

tgrid = expand.grid(
  trees = 100, #50, 100, and 150 in default 
  min_n = 1, #fixed at 1 as default 
  tree_depth = c(1,2,3,4), #1, 2, and 3 in default 
  learn_rate = c(0.01, 0.1, 0.2, 0.3, 0.4), #0.3 and 0.4 in default 
  loss_reduction = 0, #fixed at 0 in default 
  sample_size = c(0.5, 0.8, 1)) #0.5, 0.75, and 1 in default, 

xgboost_recipe <- 
  recipe(formula = Above_Median ~ ., data = train) %>% 
  #step_novel(all_nominal(), -all_outcomes()) %>% 
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>% 
  step_zv(all_predictors()) 

xgboost_spec <- 
  boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(), 
    loss_reduction = tune(), sample_size = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("xgboost") 
                                  
xgboost_workflow <- 
  workflow() %>% 
  add_recipe(xgboost_recipe) %>% 
  add_model(xgboost_spec) 

set.seed(1234)
xgb_res <-
  tune_grid(xgboost_workflow, 
            resamples = folds, 
            grid = tgrid,
            control = ctrl_grid)

end_time = Sys.time()
end_time-start_time
```

### Stacking
Be patient as this next chunk can take some time to run. Note you will likely see warning (not error) messages after you run this as tree models that produce identical results are removed.  
```{r}
ames_stacks = stacks() %>%
  add_candidates(tree_res) %>%
  add_candidates(rf_res) %>% 
  add_candidates(neural_res) %>%
  add_candidates(xgb_res)
```

Blend the predictions by fitting a Lasso model to the stack. Each model in the stack receives a coefficient value (can be zero as this is Lasso).  
```{r}
ames_blend = 
  ames_stacks %>% 
  blend_predictions(metric = metric_set(accuracy)) #fits a Lasso model to the stack  
  #setting the metric in the above line is extremely important!!
```

Look at results
```{r}
autoplot(ames_blend, type = "weights")
```





```{r}
ames_blend$model_defs
```


Fit the stack to training data
```{r}
#Fit the stack on the training set
ames_blend <-
  ames_blend %>%
  fit_members()
```

Predictions  
```{r}
trainpredstack = predict(ames_blend, train)
head(trainpredstack)
```

Confusion matrix
```{r}
confusionMatrix(trainpredstack$.pred_class, train$Above_Median, 
                positive = "Yes")
```
Predictions  
```{r}
testpredstack = predict(ames_blend, test)
head(testpredstack)
```

Confusion matrix
```{r}
confusionMatrix(testpredstack$.pred_class, test$Above_Median, 
                positive = "Yes")
```

Compare model performance on test set  
```{r}
test = test %>% bind_cols(predict(ames_blend,.))
test
```


```{r}
#compare the results of the stacked model to the constituent models
member_testpreds =  
  test %>%
  select(Above_Median) %>%
  bind_cols(predict(ames_blend, test, members = TRUE))
```

```{r}
map_dfr(member_testpreds, accuracy, truth = Above_Median, data = member_testpreds) %>%
  mutate(member = colnames(member_testpreds))
```

```{r}
saveRDS(ames_blend, "./stacked_ames.rds")
```

